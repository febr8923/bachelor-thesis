Python from: /users/fbrunne/projects/deps/python311/bin/python3
/users/fbrunne/projects/deps/patch-venv/lib/python3.11/site-packages/torch/__init__.py
/users/fbrunne/projects/deps/patch-venv/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/fbrunne/projects/deps/patch-venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention
heyy
model_loc: gpu, exec_loc: gpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:05,  1.26it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:03,  1.64it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:01<00:02,  1.79it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:02<00:02,  1.89it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:02<00:01,  1.92it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:03<00:01,  1.96it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:03<00:00,  2.00it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  2.12it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.93it/s]
/users/fbrunne/projects/deps/patch-venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention
--------
heyy
model_loc: gpu, exec_loc: gpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:03,  2.00it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:02,  2.03it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:01<00:02,  2.04it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:01<00:01,  2.04it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:02<00:01,  2.05it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:02<00:00,  2.05it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:03<00:00,  2.05it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:03<00:00,  2.28it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:03<00:00,  2.13it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
--------
finished warm-up
model_loc: cpu, exec_loc: cpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  6.66it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:00,  6.30it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  6.21it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  6.17it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  6.15it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:00<00:00,  6.15it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  6.16it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.81it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.43it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
--------
model_loc: cpu, exec_loc: cpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  6.10it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.85it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.80it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.81it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.83it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.85it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.87it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.50it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.09it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
--------
model_loc: cpu, exec_loc: cpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:00,  7.14it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:00,  6.72it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  6.61it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  6.53it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  6.50it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:00<00:00,  6.46it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  6.45it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  7.08it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.75it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
--------
model_loc: cpu, exec_loc: cpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:00,  7.24it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:00,  6.78it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  6.65it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  6.57it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  6.54it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:00<00:00,  6.49it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  6.48it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  7.13it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.80it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
--------
model_loc: cpu, exec_loc: cpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:00,  7.13it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:00,  6.15it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  6.26it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  6.30it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  6.34it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:00<00:00,  6.34it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  6.35it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  7.00it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.59it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
--------
model_loc: cpu, exec_loc: gpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:00,  7.15it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:00,  6.70it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  6.60it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  6.53it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  6.50it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:00<00:00,  6.48it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  6.48it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  7.14it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.78it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
--------
model_loc: cpu, exec_loc: gpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  6.83it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:00,  6.43it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  6.33it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  6.28it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  6.26it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:00<00:00,  6.24it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  6.23it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.92it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.54it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
--------
model_loc: cpu, exec_loc: gpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  6.78it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:00,  6.40it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  6.29it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  6.25it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  6.26it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:00<00:00,  6.24it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  6.25it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.93it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.53it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
--------
model_loc: cpu, exec_loc: gpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  6.76it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:00,  6.40it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  6.32it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  6.29it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  6.28it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:00<00:00,  5.68it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.86it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.56it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.29it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
--------
model_loc: cpu, exec_loc: gpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  6.80it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:00,  6.42it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  6.33it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  6.30it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  6.30it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:00<00:00,  6.29it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  6.29it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.91it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.55it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
--------
model_loc: gpu, exec_loc: cpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  6.74it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:00,  6.36it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  6.26it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  6.24it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  6.23it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:00<00:00,  6.22it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  6.24it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.88it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.50it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
--------
model_loc: gpu, exec_loc: cpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:00,  7.10it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:00,  6.66it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  6.53it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  6.48it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  6.47it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:00<00:00,  6.44it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  6.43it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  7.06it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.72it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
--------
model_loc: gpu, exec_loc: cpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  6.60it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:00,  6.27it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  6.19it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  6.17it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.50it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.71it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.87it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.55it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.16it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
--------
model_loc: gpu, exec_loc: cpu
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:00,  7.11it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:00,  6.64it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  6.52it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  6.46it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  6.45it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:00<00:00,  6.43it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  6.43it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  7.05it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.71it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1059699.0 ON nid007666 CANCELLED AT 2025-11-06T14:08:38 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 1059699 ON nid007666 CANCELLED AT 2025-11-06T14:08:38 DUE TO TIME LIMIT ***
