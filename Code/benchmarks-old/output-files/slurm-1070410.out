Python from: /users/fbrunne/projects/deps/python311/bin/python3
/users/fbrunne/projects/deps/patch-venv/lib/python3.11/site-packages/torch/__init__.py
/users/fbrunne/projects/deps/patch-venv/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/fbrunne/projects/deps/patch-venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Try importing flash-attention for faster inference...
Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention
Loading model Qwen/Qwen-7B...
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:02<00:19,  2.83s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:04<00:14,  2.38s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:07<00:13,  2.68s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:09<00:09,  2.42s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:11<00:06,  2.19s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:14<00:04,  2.29s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:15<00:02,  2.07s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:17<00:00,  1.97s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:17<00:00,  2.20s/it]
/users/fbrunne/projects/deps/patch-venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/users/fbrunne/projects/deps/patch-venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/fbrunne/projects/deps/patch-venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:509: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
Model loaded successfully
model_loc: cpu, exec_loc: cpu
--------
model_loc: cpu, exec_loc: cpu
--------
finished warm-up
model_loc: cpu, exec_loc: cpu
--------
model_loc: cpu, exec_loc: cpu
--------
model_loc: cpu, exec_loc: cpu
--------
model_loc: cpu, exec_loc: cpu
--------
model_loc: cpu, exec_loc: cpu
--------
model_loc: cpu, exec_loc: gpu
--------
model_loc: cpu, exec_loc: gpu
--------
model_loc: cpu, exec_loc: gpu
--------
model_loc: cpu, exec_loc: gpu
--------
model_loc: cpu, exec_loc: gpu
--------
model_loc: gpu, exec_loc: cpu
--------
model_loc: gpu, exec_loc: cpu
--------
model_loc: gpu, exec_loc: cpu
--------
model_loc: gpu, exec_loc: cpu
--------
model_loc: gpu, exec_loc: cpu
--------
model_loc: gpu, exec_loc: gpu
--------
model_loc: gpu, exec_loc: gpu
--------
model_loc: gpu, exec_loc: gpu
--------
model_loc: gpu, exec_loc: gpu
--------
model_loc: gpu, exec_loc: gpu
--------
