Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
finished warm-up
model_loc: cpu, exec_loc: cpu
Allocated memory: 0.00MB
Reserved memory: 0.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:03,  2.07it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:03,  1.73it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:01<00:02,  1.67it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:02<00:02,  1.69it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:02<00:01,  1.69it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:03<00:01,  1.70it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:04<00:00,  1.70it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.97it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.81it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 0.00MB
Reserved memory: 0.00MB
--------
model_loc: cpu, exec_loc: gpu
Allocated memory: 0.00MB
Reserved memory: 0.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.98it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.26it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.07it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.00it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  4.97it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  4.95it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  4.94it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.28it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.14it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 29458.53MB
Reserved memory: 29460.00MB
--------
model_loc: gpu, exec_loc: cpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.62it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.32it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.25it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.24it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.27it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.29it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.31it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.72it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.46it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 32.00MB
Reserved memory: 29492.00MB
--------
model_loc: gpu, exec_loc: gpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.53it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.27it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.23it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.22it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.23it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.25it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.28it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.70it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.43it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Allocated memory: 29490.53MB
Reserved memory: 29492.00MB
--------
Traceback (most recent call last):
  File "/users/fbrunne/projects/benchmarks/run.py", line 314, in <module>
    df.to_csv(f"results/results-{model_name}.csv", mode='a', header=True, index=True)
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/core/generic.py", line 3986, in to_csv
    return DataFrameRenderer(formatter).to_csv(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        path_or_buf,
        ^^^^^^^^^^^^
    ...<14 lines>...
        storage_options=storage_options,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/io/formats/format.py", line 1014, in to_csv
    csv_formatter.save()
    ~~~~~~~~~~~~~~~~~~^^
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/io/formats/csvs.py", line 251, in save
    with get_handle(
         ~~~~~~~~~~^
        self.filepath_or_buffer,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
        storage_options=self.storage_options,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ) as handles:
    ^
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/io/common.py", line 749, in get_handle
    check_parent_directory(str(handle))
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/io/common.py", line 616, in check_parent_directory
    raise OSError(rf"Cannot save file into a non-existent directory: '{parent}'")
OSError: Cannot save file into a non-existent directory: 'results/results-Qwen'
srun: error: nid007658: task 0: Exited with exit code 1
srun: Terminating StepId=920511.0
