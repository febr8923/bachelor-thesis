Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
finished warm-up
model_loc: cpu, exec_loc: cpu
Allocated memory: 0.00MB
Reserved memory: 0.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:03,  2.12it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:03,  1.84it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:01<00:02,  1.81it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:02<00:02,  1.77it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:02<00:01,  1.76it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:03<00:01,  1.73it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:04<00:00,  1.55it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.85it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.78it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 0.00MB
Reserved memory: 0.00MB
--------
model_loc: cpu, exec_loc: gpu
Allocated memory: 0.00MB
Reserved memory: 0.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.72it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.23it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.09it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.05it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.01it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.00it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.00it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.35it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.18it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 29458.53MB
Reserved memory: 29460.00MB
--------
model_loc: gpu, exec_loc: cpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.42it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.37it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.38it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.39it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.42it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.43it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.44it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.92it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.60it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 32.00MB
Reserved memory: 29492.00MB
--------
model_loc: gpu, exec_loc: gpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.61it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.54it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.55it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.55it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.55it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.54it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.55it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.04it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.73it/s]
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:06,  1.03it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:02<00:08,  1.44s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:04<00:08,  1.65s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:07<00:08,  2.08s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:09<00:06,  2.09s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.70s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:12<00:01,  1.72s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.57s/it]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Allocated memory: 14763.28MB
Reserved memory: 14764.00MB
--------
Traceback (most recent call last):
  File "/users/fbrunne/projects/benchmarks/run.py", line 321, in <module>
    df.to_csv(f"results/results-{model_name}.csv", mode='a', header=True, index=True)
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/core/generic.py", line 3986, in to_csv
    return DataFrameRenderer(formatter).to_csv(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        path_or_buf,
        ^^^^^^^^^^^^
    ...<14 lines>...
        storage_options=storage_options,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/io/formats/format.py", line 1014, in to_csv
    csv_formatter.save()
    ~~~~~~~~~~~~~~~~~~^^
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/io/formats/csvs.py", line 251, in save
    with get_handle(
         ~~~~~~~~~~^
        self.filepath_or_buffer,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
        storage_options=self.storage_options,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ) as handles:
    ^
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/io/common.py", line 749, in get_handle
    check_parent_directory(str(handle))
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/io/common.py", line 616, in check_parent_directory
    raise OSError(rf"Cannot save file into a non-existent directory: '{parent}'")
OSError: Cannot save file into a non-existent directory: 'results/results-Qwen'
srun: error: nid007658: task 0: Exited with exit code 1
srun: Terminating StepId=920527.0
