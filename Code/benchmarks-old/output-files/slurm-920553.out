The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
model_loc: gpu, exec_loc: gpu
Allocated memory: 0.00MB
Reserved memory: 0.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:02<00:19,  2.80s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:06<00:19,  3.18s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:10<00:17,  3.52s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:14<00:14,  3.74s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:18<00:11,  3.75s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:21<00:07,  3.64s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:24<00:03,  3.46s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:28<00:00,  3.61s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:28<00:00,  3.56s/it]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Allocated memory: 14731.28MB
Reserved memory: 14732.00MB
--------
finished warm-up
Traceback (most recent call last):
  File "/users/fbrunne/projects/benchmarks/run.py", line 311, in <module>
    df.at[0, col_name] = sum(times_mode_i_load) / n
                         ~~~~~~~~~~~~~~~~~~~~~~~^~~
ZeroDivisionError: division by zero
srun: error: nid007656: task 0: Exited with exit code 1
srun: Terminating StepId=920553.0
