A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- cpp_kernels.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- qwen_generation_utils.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
- cpp_kernels.py
- qwen_generation_utils.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
model_loc: gpu, exec_loc: gpu
Allocated memory: 0.00MB
Reserved memory: 0.00MB
Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]Downloading shards:  12%|█▎        | 1/8 [00:07<00:54,  7.75s/it]Downloading shards:  25%|██▌       | 2/8 [00:20<01:02, 10.50s/it]Downloading shards:  38%|███▊      | 3/8 [00:30<00:50, 10.19s/it]Downloading shards:  50%|█████     | 4/8 [00:38<00:38,  9.57s/it]Downloading shards:  62%|██████▎   | 5/8 [00:47<00:27,  9.14s/it]Downloading shards:  75%|███████▌  | 6/8 [00:54<00:17,  8.66s/it]Downloading shards:  88%|████████▊ | 7/8 [01:05<00:09,  9.30s/it]Downloading shards: 100%|██████████| 8/8 [01:13<00:00,  8.97s/it]Downloading shards: 100%|██████████| 8/8 [01:13<00:00,  9.20s/it]
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:02<00:15,  2.24s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:05<00:15,  2.59s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:07<00:12,  2.58s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:09<00:09,  2.48s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:12<00:07,  2.36s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:14<00:04,  2.40s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:16<00:02,  2.35s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:20<00:00,  2.83s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:20<00:00,  2.59s/it]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Allocated memory: 14731.28MB
Reserved memory: 14732.00MB
--------
model_loc: gpu, exec_loc: gpu
Allocated memory: 32.00MB
Reserved memory: 14766.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:05,  1.32it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:05,  1.11it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:03<00:05,  1.06s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:04<00:05,  1.27s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:05<00:03,  1.26s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:07<00:02,  1.24s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:08<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:08<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:08<00:00,  1.12s/it]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Allocated memory: 14763.28MB
Reserved memory: 14764.00MB
--------
model_loc: gpu, exec_loc: gpu
Allocated memory: 32.00MB
Reserved memory: 14766.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:05,  1.31it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:04,  1.39it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:02<00:04,  1.18it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:03<00:04,  1.01s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:04<00:03,  1.02s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:06<00:02,  1.20s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:07<00:01,  1.17s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:07<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:07<00:00,  1.03it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 14763.28MB
Reserved memory: 14764.00MB
--------
finished warm-up
model_loc: cpu, exec_loc: cpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.58it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.46it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.42it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.43it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.44it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.46it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.48it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.90it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.62it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 32.00MB
Reserved memory: 32.00MB
--------
model_loc: cpu, exec_loc: cpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.66it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.26it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.23it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.20it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.21it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.21it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.24it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.60it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.38it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 32.00MB
Reserved memory: 32.00MB
--------
model_loc: cpu, exec_loc: cpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.93it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.78it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.75it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.74it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.73it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.72it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.71it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.12it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.89it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 32.00MB
Reserved memory: 32.00MB
--------
model_loc: cpu, exec_loc: cpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.53it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.48it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.46it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.48it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.50it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.54it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.58it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.03it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.70it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 32.00MB
Reserved memory: 32.00MB
--------
model_loc: cpu, exec_loc: cpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.61it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.49it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.50it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.54it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.56it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.60it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.62it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.05it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.74it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 32.00MB
Reserved memory: 32.00MB
--------
model_loc: cpu, exec_loc: gpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.70it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.59it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.57it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.58it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.60it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.61it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.63it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.06it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.77it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 29490.53MB
Reserved memory: 29492.00MB
--------
model_loc: cpu, exec_loc: gpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.70it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.63it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.60it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.62it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.63it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.64it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.66it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.09it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.80it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 29490.53MB
Reserved memory: 29492.00MB
--------
model_loc: cpu, exec_loc: gpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.67it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.58it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.55it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.55it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.55it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.58it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.60it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.05it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.75it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 29490.53MB
Reserved memory: 29492.00MB
--------
model_loc: cpu, exec_loc: gpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.65it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.59it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.56it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.56it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.56it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.58it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.61it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.05it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.76it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 29490.53MB
Reserved memory: 29492.00MB
--------
model_loc: cpu, exec_loc: gpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.64it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.53it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.52it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.52it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.52it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.54it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.57it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.01it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.71it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 29490.53MB
Reserved memory: 29492.00MB
--------
model_loc: gpu, exec_loc: cpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.65it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.56it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.54it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.55it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.55it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.57it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.60it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.04it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.74it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 32.00MB
Reserved memory: 29492.00MB
--------
model_loc: gpu, exec_loc: cpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  6.13it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.88it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.80it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.79it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.78it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.78it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.78it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.23it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.97it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 32.00MB
Reserved memory: 29492.00MB
--------
model_loc: gpu, exec_loc: cpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  6.00it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.78it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.73it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.73it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.72it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.71it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.70it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.13it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.88it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 32.00MB
Reserved memory: 29492.00MB
--------
model_loc: gpu, exec_loc: cpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  6.04it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.83it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.79it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.75it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.73it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.71it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.28it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.81it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.72it/s]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 920579.0 ON nid007656 CANCELLED AT 2025-10-03T14:55:43 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 920579 ON nid007656 CANCELLED AT 2025-10-03T14:55:43 DUE TO TIME LIMIT ***
