The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
model_loc: gpu, exec_loc: gpu
Allocated memory: 0.00MB
Reserved memory: 0.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:02<00:20,  2.96s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:06<00:20,  3.39s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:09<00:16,  3.33s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:13<00:13,  3.34s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:17<00:10,  3.59s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:20<00:07,  3.54s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:24<00:03,  3.58s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:28<00:00,  3.68s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:28<00:00,  3.54s/it]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Allocated memory: 14731.28MB
Reserved memory: 14732.00MB
--------
model_loc: gpu, exec_loc: gpu
Allocated memory: 32.00MB
Reserved memory: 14766.00MB
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:07,  1.03s/it]slurmstepd: error: *** STEP 920657.0 ON nid007658 CANCELLED AT 2025-10-03T14:56:58 ***
slurmstepd: error: *** JOB 920657 ON nid007658 CANCELLED AT 2025-10-03T14:56:58 ***
