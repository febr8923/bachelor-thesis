Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
finished warm-up
model_loc: cpu, exec_loc: cpu
Allocated memory: 0.00MB
Reserved memory: 0.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:03,  1.78it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:03,  1.62it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:01<00:02,  1.67it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:02<00:02,  1.71it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:03<00:01,  1.52it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:03<00:01,  1.60it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:04<00:00,  1.65it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.93it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.74it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 0.00MB
Reserved memory: 0.00MB
--------
model_loc: cpu, exec_loc: gpu
Allocated memory: 0.00MB
Reserved memory: 0.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  6.31it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.46it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.25it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.14it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.08it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.06it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.05it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.39it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.27it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 29458.53MB
Reserved memory: 29460.00MB
--------
model_loc: gpu, exec_loc: cpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.88it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.58it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.52it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.49it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.48it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.50it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.52it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.00it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.71it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Allocated memory: 32.00MB
Reserved memory: 29492.00MB
--------
model_loc: gpu, exec_loc: gpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:06,  1.03it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:02<00:07,  1.20s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:04<00:07,  1.59s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:05<00:05,  1.40s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:07<00:05,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.95s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.90s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.54s/it]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Allocated memory: 14763.28MB
Reserved memory: 14764.00MB
--------
gpu_cpu,cpu_gpu,gpu_gpu,cpu_cpu
93.7830739589408,0.3387794520240277,0.002256212057545781,0.23519150190986693
93.7830739589408,0.3387794520240277,0.002256212057545781,0.23519150190986693
93.7830739589408,0.3387794520240277,0.002256212057545781,0.23519150190986693
242.7252515349537,15.117769161006436,21.221030607121065,114.34563080011867
242.7252515349537,15.117769161006436,21.221030607121065,114.34563080011867
242.7252515349537,15.117769161006436,21.221030607121065,114.34563080011867
336.5083254938945,15.456548613030463,21.22328681917861,114.58082230202854
336.5083254938945,15.456548613030463,21.22328681917861,114.58082230202854
336.5083254938945,15.456548613030463,21.22328681917861,114.58082230202854

Traceback (most recent call last):
  File "/users/fbrunne/projects/benchmarks/run.py", line 325, in <module>
    df.to_csv(f"results/results-{model_name}.csv", mode='a', header=True, index=True)
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/core/generic.py", line 3986, in to_csv
    return DataFrameRenderer(formatter).to_csv(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        path_or_buf,
        ^^^^^^^^^^^^
    ...<14 lines>...
        storage_options=storage_options,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/io/formats/format.py", line 1014, in to_csv
    csv_formatter.save()
    ~~~~~~~~~~~~~~~~~~^^
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/io/formats/csvs.py", line 251, in save
    with get_handle(
         ~~~~~~~~~~^
        self.filepath_or_buffer,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
        storage_options=self.storage_options,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ) as handles:
    ^
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/io/common.py", line 749, in get_handle
    check_parent_directory(str(handle))
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/users/fbrunne/miniconda3/envs/rmm_dev/lib/python3.13/site-packages/pandas/io/common.py", line 616, in check_parent_directory
    raise OSError(rf"Cannot save file into a non-existent directory: '{parent}'")
OSError: Cannot save file into a non-existent directory: 'results/results-Qwen'
srun: error: nid007656: task 0: Exited with exit code 1
srun: Terminating StepId=920665.0
