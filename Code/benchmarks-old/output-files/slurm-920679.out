Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
finished warm-up
model_loc: cpu, exec_loc: cpu
Allocated memory: 0.00MB
Reserved memory: 0.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:03,  2.10it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:03,  1.86it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:01<00:02,  1.78it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:02<00:02,  1.77it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:02<00:01,  1.63it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:03<00:01,  1.67it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:04<00:00,  1.69it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.98it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.83it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 0.00MB
Reserved memory: 0.00MB
--------
model_loc: cpu, exec_loc: gpu
Allocated memory: 0.00MB
Reserved memory: 0.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.96it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.33it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.13it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.07it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.04it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.01it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  4.99it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.35it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.21it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Allocated memory: 29458.53MB
Reserved memory: 29460.00MB
--------
model_loc: gpu, exec_loc: cpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  6.07it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.72it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.65it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.60it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.60it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.59it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.57it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.02it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.78it/s]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Allocated memory: 32.00MB
Reserved memory: 29492.00MB
--------
model_loc: gpu, exec_loc: gpu
Allocated memory: 32.00MB
Reserved memory: 32.00MB
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:04<00:31,  4.44s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:06<00:19,  3.20s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:08<00:13,  2.67s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:10<00:09,  2.33s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:13<00:07,  2.35s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:15<00:04,  2.41s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:18<00:02,  2.51s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:18<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:18<00:00,  2.33s/it]
Both `max_new_tokens` (=512) and `max_length`(=1) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Allocated memory: 14763.28MB
Reserved memory: 14764.00MB
--------
