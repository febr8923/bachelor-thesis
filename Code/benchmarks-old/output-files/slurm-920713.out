A new version of the following files was downloaded from https://huggingface.co/baichuan-inc/Baichuan2-7B-Base:
- tokenization_baichuan.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/baichuan-inc/Baichuan2-7B-Base:
- configuration_baichuan.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/baichuan-inc/Baichuan2-7B-Base:
- quantizer.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/baichuan-inc/Baichuan2-7B-Base:
- generation_utils.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/baichuan-inc/Baichuan2-7B-Base:
- modeling_baichuan.py
- quantizer.py
- generation_utils.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
finished warm-up
model_loc: cpu, exec_loc: cpu
Allocated memory: 0.00MB
Reserved memory: 0.00MB
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:  50%|█████     | 1/2 [00:28<00:28, 28.94s/it]Downloading shards: 100%|██████████| 2/2 [04:17<00:00, 146.16s/it]Downloading shards: 100%|██████████| 2/2 [04:17<00:00, 128.58s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]
Allocated memory: 0.00MB
Reserved memory: 0.00MB
--------
Traceback (most recent call last):
  File "/users/fbrunne/projects/benchmarks/run.py", line 294, in <module>
    res = run_llm(model_loc=mode[0], exec_loc=mode[1], model_name=model_name)
  File "/users/fbrunne/projects/benchmarks/run.py", line 249, in run_llm
    output = model.generate(**input_batch, max_length=1)
  File "/user-environment/env/default/lib/python3.13/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/user-environment/env/default/lib/python3.13/site-packages/transformers/generation/utils.py", line 2108, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/user-environment/env/default/lib/python3.13/site-packages/transformers/generation/utils.py", line 1411, in _validate_generated_length
    raise ValueError(
    ...<3 lines>...
    )
ValueError: Input length of input_ids is 4, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.
srun: error: nid007656: task 0: Exited with exit code 1
srun: Terminating StepId=920713.0
