Using LD_LIBRARY_PATH=/users/fbrunne/miniconda3/envs/rmm_dev/lib:/user-environment/env/default/lib64:/user-environment/env/default/lib:/opt/cray/libfabric/1.15.2.0/lib64:/opt/cray/libfabric/1.15.2.0/lib:/user-environment/env/default/lib/python3.13/site-packages/faiss:/users/fbrunne/miniconda3/envs/rmm_dev/lib
/user-environment/env/default/lib/python3.13/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
model_loc: gpu, exec_loc: gpu
Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]Downloading shards:  12%|█▎        | 1/8 [00:35<04:05, 35.05s/it]Downloading shards:  25%|██▌       | 2/8 [00:38<01:38, 16.40s/it]Downloading shards:  38%|███▊      | 3/8 [00:41<00:51, 10.39s/it]Downloading shards:  50%|█████     | 4/8 [00:44<00:30,  7.53s/it]Downloading shards:  62%|██████▎   | 5/8 [00:47<00:17,  5.95s/it]Downloading shards:  75%|███████▌  | 6/8 [00:51<00:09,  4.99s/it]Downloading shards:  88%|████████▊ | 7/8 [00:54<00:04,  4.37s/it]Downloading shards: 100%|██████████| 8/8 [00:56<00:00,  3.80s/it]Downloading shards: 100%|██████████| 8/8 [00:56<00:00,  7.09s/it]
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:03,  2.12it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:02,  2.02it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:01<00:02,  2.01it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:01<00:01,  2.00it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:02<00:01,  2.00it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:02<00:01,  2.00it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:03<00:00,  1.98it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:03<00:00,  2.19it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:03<00:00,  2.08it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
--------
model_loc: gpu, exec_loc: gpu
Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 8/8 [00:00<00:00, 2482.94it/s]
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:03,  2.05it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:02,  2.02it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:01<00:02,  2.01it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:01<00:01,  2.01it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:02<00:01,  2.00it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:02<00:00,  2.00it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:03<00:00,  2.00it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:03<00:00,  2.20it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:03<00:00,  2.08it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
--------
finished warm-up
model_loc: cpu, exec_loc: cpu
Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 8/8 [00:00<00:00, 3944.80it/s]
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:00, 11.65it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00, 12.89it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:00<00:00, 13.55it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:00<00:00, 13.88it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:00<00:00, 13.49it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
--------
Traceback (most recent call last):
  File "/users/fbrunne/projects/benchmarks/run.py", line 295, in <module>
    res = run_llm(model_loc=mode[0], exec_loc=mode[1], model_name=model_name)
  File "/users/fbrunne/projects/benchmarks/run.py", line 250, in run_llm
    output = model.generate(**input_batch, max_length=10)
  File "/users/fbrunne/.cache/huggingface/modules/transformers_modules/Qwen/Qwen-7B/ef3c5c9c57b252f3149c1408daf4d649ec8b6c85/modeling_qwen.py", line 1259, in generate
    return super().generate(
           ~~~~~~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<7 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "/user-environment/env/default/lib/python3.13/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/user-environment/env/default/lib/python3.13/site-packages/transformers/generation/utils.py", line 2255, in generate
    result = self._sample(
        input_ids,
    ...<5 lines>...
        **model_kwargs,
    )
  File "/user-environment/env/default/lib/python3.13/site-packages/transformers/generation/utils.py", line 3257, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/user-environment/env/default/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/user-environment/env/default/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/fbrunne/.cache/huggingface/modules/transformers_modules/Qwen/Qwen-7B/ef3c5c9c57b252f3149c1408daf4d649ec8b6c85/modeling_qwen.py", line 1043, in forward
    transformer_outputs = self.transformer(
        input_ids,
    ...<11 lines>...
        return_dict=return_dict,
    )
  File "/user-environment/env/default/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/user-environment/env/default/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/fbrunne/.cache/huggingface/modules/transformers_modules/Qwen/Qwen-7B/ef3c5c9c57b252f3149c1408daf4d649ec8b6c85/modeling_qwen.py", line 891, in forward
    outputs = block(
        hidden_states,
    ...<7 lines>...
        output_attentions=output_attentions,
    )
  File "/user-environment/env/default/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/user-environment/env/default/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/fbrunne/.cache/huggingface/modules/transformers_modules/Qwen/Qwen-7B/ef3c5c9c57b252f3149c1408daf4d649ec8b6c85/modeling_qwen.py", line 610, in forward
    attn_outputs = self.attn(
        layernorm_output,
    ...<5 lines>...
        output_attentions=output_attentions,
    )
  File "/user-environment/env/default/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/user-environment/env/default/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/fbrunne/.cache/huggingface/modules/transformers_modules/Qwen/Qwen-7B/ef3c5c9c57b252f3149c1408daf4d649ec8b6c85/modeling_qwen.py", line 519, in forward
    raise Exception(_ERROR_INPUT_CPU_QUERY_WITH_FLASH_ATTN_ACTIVATED)
Exception: We detect you have activated flash attention support, but running model computation on CPU. Please make sure that your input data has been placed on GPU. If you actually want to run CPU computation, please following the readme and set device_map="cpu" to disable flash attention when loading the model (calling AutoModelForCausalLM.from_pretrained).
检测到您的模型已激活了flash attention支持，但正在执行CPU运算任务。如使用flash attention，请您确认模型输入已经传到GPU上。如果您确认要执行CPU运算，请您在载入模型（调用AutoModelForCausalLM.from_pretrained）时，按照readme说法，指定device_map="cpu"以禁用flash attention。

srun: error: nid007669: task 0: Exited with exit code 1
srun: Terminating StepId=965226.0
