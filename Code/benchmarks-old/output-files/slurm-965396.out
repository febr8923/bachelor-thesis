Using LD_LIBRARY_PATH=/users/fbrunne/miniconda3/envs/rmm_dev/lib:/user-environment/env/default/lib64:/user-environment/env/default/lib:/opt/cray/libfabric/1.15.2.0/lib64:/opt/cray/libfabric/1.15.2.0/lib:/user-environment/env/default/lib/python3.13/site-packages/faiss:/users/fbrunne/miniconda3/envs/rmm_dev/lib
/user-environment/env/default/lib/python3.13/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
model_loc: gpu, exec_loc: gpu
Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 8/8 [00:00<00:00, 1200.99it/s]
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:02<00:14,  2.06s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:11,  1.97s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:09,  1.84s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  1.87s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:09<00:05,  1.82s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:11<00:04,  2.07s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:13<00:02,  2.08s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:15<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:15<00:00,  1.91s/it]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
--------
model_loc: gpu, exec_loc: gpu
Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 8/8 [00:00<00:00, 11570.49it/s]
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.33it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  4.94it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:01,  4.83it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  4.79it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:01<00:00,  4.78it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  4.77it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  4.77it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.08it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  4.93it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
--------
finished warm-up
model_loc: cpu, exec_loc: cpu
Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 8/8 [00:00<00:00, 11177.36it/s]
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.36it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.30it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.30it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.31it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.33it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.36it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.37it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.80it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.50it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
--------
model_loc: cpu, exec_loc: gpu
Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 8/8 [00:00<00:00, 11687.37it/s]
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.36it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.31it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.31it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.33it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.35it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.38it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.38it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.78it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.51it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
--------
model_loc: gpu, exec_loc: cpu
Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 8/8 [00:00<00:00, 11650.84it/s]
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.41it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.33it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.32it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.33it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.34it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  5.35it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  5.37it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.77it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.50it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- tokenization_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- configuration_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B:
- modeling_qwen.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
--------
model_loc: gpu, exec_loc: gpu
Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 8/8 [00:00<00:00, 11229.73it/s]
Your device support faster inference by passing bf16=True in "AutoModelForCausalLM.from_pretrained".
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  5.70it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  5.59it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:00,  5.54it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  5.51it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:00<00:00,  5.49it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  2.30it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:02<00:00,  2.83it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  3.50it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  3.71it/s]
Both `max_new_tokens` (=512) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
--------
